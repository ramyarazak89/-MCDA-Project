# -*- coding: utf-8 -*-
"""main_experiment.py.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wa-MHI6c5LdfIoo0pAQ-z8lKGeiKwkaz
"""

import torch
import numpy as np
import matplotlib.pyplot as plt
from mcda_architecture import MCDA

class BanditEnvironment:
    """Simulates a Multi-Armed Bandit environment."""
    def __init__(self, num_bandits):
        self.probabilities = np.random.rand(num_bandits)
        print(f"Bandit Environment created. True probabilities: {np.round(self.probabilities, 2)}")

    def pull_lever(self, bandit_index):
        """Returns a reward of 1.0 with the bandit's specific probability, otherwise 0.0."""
        return 1.0 if np.random.rand() < self.probabilities[bandit_index] else 0.0

# --- Simulation Parameters ---
NUM_BANDITS = 5
STATE_SIZE = 2  # A simplified state representation (e.g., 0 for 'no reward', 1 for 'reward received')
INPUT_FEATURES = 10
CDA_NEURONS = 30
NUM_EPISODES = 500
EPSILON = 0.2  # Exploration rate

# 1. Setup Environment and Architecture
env = BanditEnvironment(NUM_BANDITS)
system = MCDA(
    state_size=STATE_SIZE,
    num_actions=NUM_BANDITS,
    num_inputs=INPUT_FEATURES,
    cda_neurons=CDA_NEURONS
)

# 2. Training Loop
total_rewards = []
state = 0  # Initial state

print(f"\n--- Starting training for {NUM_EPISODES} episodes ---")
for episode in range(NUM_EPISODES):
    # For this problem, the input vector to CDA is not critical, so we create a dummy one
    dummy_inputs = [torch.bernoulli(torch.full((INPUT_FEATURES,), 0.5)) for _ in range(NUM_BANDITS)]

    # The MCDA system takes a full step
    action, cda_output = system.step(dummy_inputs, state, EPSILON)

    # Get a real reward from the environment based on the chosen action
    reward = env.pull_lever(action)
    total_rewards.append(reward)

    # Determine the next state based on the reward
    next_state = 1 if reward > 0 else 0
    done = True  # Each episode in the bandit problem is a single step

    # Add the experience to the IQ Layer's memory and train the DQN
    system.iq_layer.memory.add(state, action, reward, next_state, done)
    loss = system.iq_layer.train_from_memory()

    state = next_state

    # Periodically update the target network for stable learning
    if episode > 0 and episode % 20 == 0:
      system.iq_layer.target_net.load_state_dict(system.iq_layer.policy_net.state_dict())

print("--- Training finished ---")

# 3. Analyze and Display Results
# Check the learned policy for the initial state (state 0)
state_0_tensor = torch.zeros(1, STATE_SIZE)
state_0_tensor[0, 0] = 1.0
with torch.no_grad():
    q_values = system.iq_layer.policy_net(state_0_tensor).flatten().numpy()

print(f"\nFinal Learned Q-Values for each action: {np.round(q_values, 2)}")

best_bandit_actual = np.argmax(env.probabilities)
best_bandit_learned = np.argmax(q_values)

print(f"Actual best bandit: #{best_bandit_actual} | Learned best bandit: #{best_bandit_learned}")

if best_bandit_actual == best_bandit_learned:
    print("\n✅ Success! The MCDA architecture correctly identified the optimal policy.")
else:
    print("\n⚠️ The agent did not converge to the optimal policy. Consider more episodes or different hyperparameters.")

# 4. Plot Cumulative Reward
plt.figure(figsize=(12, 7))
plt.plot(np.cumsum(total_rewards))
plt.title('Cumulative Reward Over Time')
plt.xlabel('Episode')
plt.ylabel('Total Reward')
plt.grid(True)
plt.show()